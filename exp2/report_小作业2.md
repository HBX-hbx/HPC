<center><font size=6>小作业二：MPI Allreduce</font></center>

<center><font size=4>计04 何秉翔 2020010944</font></center>

### 1. `Ring_Allreduce` 实现

我们将 `all_reduce` 的过程分成两个阶段，下面两个阶段分别介绍。

设总共的进程数为 $p$，当前进程为 $i\in [0,\ p )$，需要进行 `all_reduce` 的 `float` 个数为 $n$，为了防止死锁，所有的通信以**非阻塞方式**进行。

#### 1.1 stage 1: `reduce scatter`

1. 首先将当前进程 $i$ 的数据 `sendbuf` 分为 $p$ 个数据块，数据块编号 $j\in [0,\ p)$，每个数据块的大小为 $(n + p - 1) / p$，由于 $p$ 不一定整除 $n$，所以至多最后一个数据块未填满。
2. 对于当前进程 $i$，接着连续进行 $p - 1$ 步操作，对于第 $k\in [0,\ p - 1)$ 步操作，做如下的事情：
   + **非阻塞：**将自己的第 $(i - k + p)\ \%  \ p$ 个数据块发送给第 $(i + 1) \ \% \ p$ 个进程。
   + **非阻塞：**从第 $(i - 1 + p)\ \% \ p$ 个进程那接受数据到一个缓冲 `buf` 中。
   + 使用 `MPI_Waitall` 等待通信完成。
   + 将接受的 `buf` 中的数据**累加**到 `sendbuf` 对应位置中，以待下一步操作。
3. 在第 $p - 1$ 步结束后，第 $i$ 个进程的第 $(i + 1)\ \% \ p$ 个数据块保存了所有进程的同位置数据块的累加和，如 `ppt` 图所示：

![image-20230321213030984](C:\Users\Alexander\AppData\Roaming\Typora\typora-user-images\image-20230321213030984.png)

#### 1.2 stage 2: `all gather`

1. 对于当前进程 $i$，接着连续进行 $p - 1$ 步操作，对于第 $k\in [0,\ p - 1)$ 步操作，做如下的事情：
   + **非阻塞：**将自己的第 $(i + 1 - k + p)\ \%  \ p$ 个数据块发送给第 $(i + 1) \ \% \ p$ 个进程。
   + **非阻塞：**从第 $(i - 1 + p)\ \% \ p$ 个进程那接受数据到一个缓冲 `buf` 中。
   + 使用 `MPI_Waitall` 等待通信完成。
   + 将接受的 `buf` 中的数据**赋值**到 `sendbuf` 对应位置中，以待下一步操作。
2. 在第 $p - 1$ 步结束后，每个进程的每个数据块都是所有进程对应位置数据块的累加和，即 `all_reduce` 目标达成，如 `ppt` 图所示：

![image-20230321213414546](C:\Users\Alexander\AppData\Roaming\Typora\typora-user-images\image-20230321213414546.png)

### 2. 通信时间测试

在此部分，我们改变使用的节点数 $N\in \{1, 2\}$，进程数 $p\in \{2, 4, 8, 16\}$，以及需要通信的 `float` 数 $n\in \{1K, 32K, 1M\}$，测得三种 `all_reduce` 通信算法运行时间如下表所示：

| 进程数 $p$ | 通信量 $n$ | 节点数 $N$ | `MPI_Allreduce` | `Naive_Allreduce` | `Ring_Allreduce` |
| :--------: | :--------: | :--------: | :-------------: | :---------------: | :--------------: |
|     2      |    $1K$    |     1      |  $0.115303 ms$  |   $0.22235 ms$    |   $0.12677 ms$   |
|     2      |   $32K$    |     1      |  $1.41175 ms$   |   $1.77609 ms$    |   $1.12869 ms$   |
|     2      |    $1M$    |     1      |  $23.1996 ms$   |   $30.3268 ms$    |   $21.9561 ms$   |
|     4      |    $1K$    |     1      |  $0.187502 ms$  |   $0.306813 ms$   |  $0.195607 ms$   |
|     4      |   $32K$    |     1      |  $2.18847 ms$   |   $3.22673 ms$    |   $1.90558 ms$   |
|     4      |    $1M$    |     1      |  $32.9582 ms$   |   $46.5607 ms$    |   $29.8168 ms$   |
|     8      |    $1K$    |     1      |  $0.233642 ms$  |   $0.31693 ms$    |  $0.224757 ms$   |
|     8      |   $32K$    |     1      |   $2.0692 ms$   |   $3.38255 ms$    |   $2.62621 ms$   |
|     8      |    $1M$    |     1      |  $46.7107 ms$   |    $57.877 ms$    |   $43.2431 ms$   |
|     16     |    $1K$    |     1      |  $0.328218 ms$  |   $0.372476 ms$   |  $0.335371 ms$   |
|     16     |   $32K$    |     1      |  $1.67368 ms$   |   $3.23829 ms$    |   $1.51073 ms$   |
|     16     |    $1M$    |     1      |  $53.1162 ms$   |   $66.7656 ms$    |   $53.4931 ms$   |
|     2      |    $1K$    |     2      |  $0.097643 ms$  |   $0.119063 ms$   |  $0.084409 ms$   |
|     2      |   $32K$    |     2      |  $0.909532 ms$  |   $1.25772 ms$    |  $0.864494 ms$   |
|     2      |    $1M$    |     2      |  $14.9008 ms$   |   $22.3426 ms$    |   $14.0672 ms$   |
|     4      |    $1K$    |     2      |  $0.201034 ms$  |   $0.291444 ms$   |  $0.214173 ms$   |
|     4      |   $32K$    |     2      |  $2.37275 ms$   |   $3.86773 ms$    |   $2.29529 ms$   |
|     4      |    $1M$    |     2      |  $31.4416 ms$   |   $45.6233 ms$    |   $26.902 ms$    |
|     8      |    $1K$    |     2      |  $0.322703 ms$  |   $0.415348 ms$   |   $0.32716 ms$   |
|     8      |   $32K$    |     2      |  $2.75309 ms$   |   $5.24294 ms$    |   $3.23659 ms$   |
|     8      |    $1M$    |     2      |  $39.0054 ms$   |   $59.1673 ms$    |   $41.8886 ms$   |
|     16     |    $1K$    |     2      |  $0.507563 ms$  |   $0.515228 ms$   |  $0.491788 ms$   |
|     16     |   $32K$    |     2      |  $2.33681 ms$   |   $4.41913 ms$    |   $1.95505 ms$   |
|     16     |    $1M$    |     2      |  $48.6776 ms$   |   $77.7277 ms$    |   $51.8548 ms$   |

可以看出：

1. 相同节点数和通信量时，运行时间基本随着进程数增加而增加，这可能由于实现中没有考虑计算和通信重叠，而是直接在非阻塞通信后进行 `MPI_Waitall`，因此进程数越多通信时间占比越大
2. 相同进程数和节点数时，运行时间随着通信量增加而增加。
3. 相同进程数和通信量时，节点间进程通信和节点内进程通信性能不同，没有太明显的对比。
